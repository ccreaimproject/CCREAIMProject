hydra:
  run:
    dir: /scratch/other/sopi/CCREAIM/logs/hydra_output/${hydra.job.name}/${now:%Y-%m-%d_%H-%M-%S}

defaults:
    - base.yaml
    - _self_
    - runs: smoke_test
    #- override hydra/launcher: submitit_slurm

<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
seed: 0
exp_name: base
run_id: ???

model: ae
model_version: base
seq_length: 1000

model_path: /scratch/other/sopi/CCREAIM/trained_models
original_data_root: /scratch/other/sopi/CCREAIM/datasets/magna-tag-a-tune
data_root: /scratch/other/sopi/CCREAIM/datasets

epochs: 1
batch_size: 8
learning_rate: 1e-3
gpus: 1



=======
# Logging
>>>>>>> 4f957bd (slurm test script and)
save_logging: true
save_model: true
wandb: true
silent: false
seed: 0
exp_name: base
run_id: 0
run_suffix: 0

=======
>>>>>>> cffcef6 (messy transformer implementation and testing imp)
=======
train: true
>>>>>>> 1a12b74 (Simplified train.py and test.py to just include the training loop itself, merge the other functionalities into main.py)

# Parameters for slurm
gres: gpu:v100:1

# Saved model destination
checkpoint: 0

# Hyperparameters for training
model: ae
seq_length: 1000
<<<<<<< HEAD
epochs: 1
batch_size: 64
=======
epochs: 10
batch_size: 8
>>>>>>> c72a75c (data padding, logging and transformer start)
learning_rate: 1e-3
shuffle: true
